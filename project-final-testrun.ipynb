{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow-gpu\n!pip install --no-deps tensorflow-io","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:20:12.525084Z","iopub.execute_input":"2023-06-14T17:20:12.525556Z","iopub.status.idle":"2023-06-14T17:20:19.063794Z","shell.execute_reply.started":"2023-06-14T17:20:12.525521Z","shell.execute_reply":"2023-06-14T17:20:19.062489Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting tensorflow-gpu\n  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting python_version>\"3.7\" (from tensorflow-gpu)\n  Downloading python_version-0.0.2-py2.py3-none-any.whl (3.4 kB)\nBuilding wheels for collected packages: tensorflow-gpu\n  Building wheel for tensorflow-gpu (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m Traceback (most recent call last):\n  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-delfyue0/tensorflow-gpu_c862fb26a5464431bb30d9802ae6f846/setup.py\", line 37, in <module>\n  \u001b[31m   \u001b[0m     raise Exception(TF_REMOVAL_WARNING)\n  \u001b[31m   \u001b[0m Exception:\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m =========================================================\n  \u001b[31m   \u001b[0m The \"tensorflow-gpu\" package has been removed!\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Please install \"tensorflow\" instead.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Other than the name, the two packages have been identical\n  \u001b[31m   \u001b[0m since TensorFlow 2.1, or roughly since Sep 2019. For more\n  \u001b[31m   \u001b[0m information, see: pypi.org/project/tensorflow-gpu\n  \u001b[31m   \u001b[0m =========================================================\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed building wheel for tensorflow-gpu\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25h  Running setup.py clean for tensorflow-gpu\nFailed to build tensorflow-gpu\n\u001b[31mERROR: Could not build wheels for tensorflow-gpu, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: tensorflow-io in /opt/conda/lib/python3.10/site-packages (0.31.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:20:44.648059Z","iopub.execute_input":"2023-06-14T17:20:44.648439Z","iopub.status.idle":"2023-06-14T17:20:44.652785Z","shell.execute_reply.started":"2023-06-14T17:20:44.648411Z","shell.execute_reply":"2023-06-14T17:20:44.651969Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"input_dir = \"/kaggle/input/cocovqa/train2014\"\n!mkdir -p /kaggle/working/VQA_preprocessed/ImagesFeatures\noutput_dir = \"/kaggle/working/VQA_preprocessed/ImagesFeatures\"","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:24:24.168552Z","iopub.execute_input":"2023-06-14T17:24:24.168924Z","iopub.status.idle":"2023-06-14T17:24:25.226634Z","shell.execute_reply.started":"2023-06-14T17:24:24.168894Z","shell.execute_reply":"2023-06-14T17:24:25.225274Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (224, 224))\n    img = tf.keras.applications.vgg19.preprocess_input(img)\n    return img, image_path","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:24:25.373372Z","iopub.execute_input":"2023-06-14T17:24:25.374491Z","iopub.status.idle":"2023-06-14T17:24:25.381849Z","shell.execute_reply.started":"2023-06-14T17:24:25.374452Z","shell.execute_reply":"2023-06-14T17:24:25.380483Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def feature_path(p, train_flag):\n    # ./data/./img/train2014/COCO_train2014_000000178619.jpg\n    directory = output_dir\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    name = p.split('/')[-1].split('.')[0] + '.npy'\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    directory = os.path.join(directory, 'train' if train_flag else 'test')\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    return os.path.join(directory, name)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:24:28.369626Z","iopub.execute_input":"2023-06-14T17:24:28.369980Z","iopub.status.idle":"2023-06-14T17:24:28.377083Z","shell.execute_reply.started":"2023-06-14T17:24:28.369948Z","shell.execute_reply":"2023-06-14T17:24:28.376164Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def extract_feature(img_path,\n                    train_flag,\n                    image_features_extract_model\n                    ):\n    # feature is in shape of [7*7, 512]\n\n    unique_img = list(set(img_path))\n    print(\"total image# to preprocess: \", len(unique_img))\n\n    image_dataset = tf.data.Dataset.from_tensor_slices(unique_img)\n    image_dataset = image_dataset.map(\n        load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n\n    for img, path in tqdm(image_dataset):\n        batch_features = image_features_extract_model(img)\n        batch_features = tf.reshape(batch_features,\n                                    (batch_features.shape[0],\n                                     -1, batch_features.shape[3]))\n        for f, p in zip(batch_features, path):\n            path_of_feature = p.numpy().decode(\"utf-8\")\n            path_of_feature = feature_path(path_of_feature, train_flag=train_flag)\n            np.save(path_of_feature, f.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:24:31.924234Z","iopub.execute_input":"2023-06-14T17:24:31.924589Z","iopub.status.idle":"2023-06-14T17:24:31.932476Z","shell.execute_reply.started":"2023-06-14T17:24:31.924562Z","shell.execute_reply":"2023-06-14T17:24:31.931575Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trains = np.load('/kaggle/input/vqa-inputs/train.npy', allow_pickle=True).tolist()\nprint(len(trains))\nfor item in trains:\n  path = item[0]\n  name = path.split('/')[-1]\n  path = os.path.join(input_dir,'train2014', name)\n  print(path)\n  break","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:24:36.428681Z","iopub.execute_input":"2023-06-14T17:24:36.429452Z","iopub.status.idle":"2023-06-14T17:24:37.583725Z","shell.execute_reply.started":"2023-06-14T17:24:36.429419Z","shell.execute_reply":"2023-06-14T17:24:37.582601Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"443757\n/kaggle/input/cocovqa/train2014/train2014/COCO_train2014_000000458752.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"trains = np.load('/kaggle/input/vqa-inputs/train.npy', allow_pickle=True).tolist()\nimg_paths = []\nfor item in trains:\n  path = item[0]\n  name = path.split('/')[-1]\n  path = os.path.join(input_dir, '', name)\n  img_paths.append(path)\n  # using vgg19 pool5 to extract image feature\n\nimage_model = tf.keras.applications.VGG19(include_top=False,\n                                          weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\nprint(\"image model ready\")\n\n# extract image features\n# Train\n\nextract_feature(img_paths,\n                True,\n                image_features_extract_model\n                )\nprint(\"train image done.\")","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:24:39.028531Z","iopub.execute_input":"2023-06-14T17:24:39.028890Z","iopub.status.idle":"2023-06-14T17:40:04.546792Z","shell.execute_reply.started":"2023-06-14T17:24:39.028859Z","shell.execute_reply":"2023-06-14T17:40:04.545794Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"image model ready\ntotal image# to preprocess:  82783\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1294/1294 [15:21<00:00,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"train image done.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for item in trains[:10]:\n  path = item[0]\n  name = path.split('/')[-1][:-3] + 'npy'\n  #org_path = os.path.join(input_dir, 'train2014', name)\n  feature_path = os.path.join(output_dir, 'train', name)\n  x = np.load(feature_path, allow_pickle=True).shape\n  print(x)\n\n  if not os.path.exists(feature_path):\n    print('error')","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:41:09.429909Z","iopub.execute_input":"2023-06-14T17:41:09.430278Z","iopub.status.idle":"2023-06-14T17:41:09.452496Z","shell.execute_reply.started":"2023-06-14T17:41:09.430248Z","shell.execute_reply":"2023-06-14T17:41:09.451368Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n(49, 512)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:41:12.643903Z","iopub.execute_input":"2023-06-14T17:41:12.644286Z","iopub.status.idle":"2023-06-14T17:41:12.649132Z","shell.execute_reply.started":"2023-06-14T17:41:12.644252Z","shell.execute_reply":"2023-06-14T17:41:12.648195Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class vocab:\n\n  def __init__(self, vocab_file):\n    self.vocab = self.load_vocab(vocab_file)\n    self.vocab2idx = {word: idx for idx, word in enumerate(self.vocab)}\n    self.vocab_size = len(self.vocab)\n\n  def load_vocab(self, vocab_file):\n    with open(vocab_file, 'r') as f:\n      vocab = [line.strip() for line in f]\n    return vocab\n\n  def word2idx(self, word):\n    if word in self.vocab2idx:\n      return self.vocab2idx[word]\n    else:\n      return self.vocab2idx['']\n\n  def idx2word(self, idx):\n    return self.vocab[idx]","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:41:15.604151Z","iopub.execute_input":"2023-06-14T17:41:15.604542Z","iopub.status.idle":"2023-06-14T17:41:15.612550Z","shell.execute_reply.started":"2023-06-14T17:41:15.604504Z","shell.execute_reply":"2023-06-14T17:41:15.611489Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"question_vocab_dir = '/kaggle/input/vqa-preprocessed/preprocessed/Questions/question_vocabs.txt'\nquestion_vocab = vocab(question_vocab_dir)\n\nanswer_vocab_dir = '/kaggle/input/vqa-preprocessed/preprocessed/Annotations/annotation_vocabs.txt'\nanswer_vocab = vocab(answer_vocab_dir)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:11.247019Z","iopub.execute_input":"2023-06-14T17:45:11.247700Z","iopub.status.idle":"2023-06-14T17:45:11.288500Z","shell.execute_reply.started":"2023-06-14T17:45:11.247668Z","shell.execute_reply":"2023-06-14T17:45:11.287581Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def load_features(features_path):\n  return np.load(features_path, allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:18.735974Z","iopub.execute_input":"2023-06-14T17:45:18.736684Z","iopub.status.idle":"2023-06-14T17:45:18.741979Z","shell.execute_reply.started":"2023-06-14T17:45:18.736648Z","shell.execute_reply":"2023-06-14T17:45:18.740976Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"max_qu_length = 30","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:22.007983Z","iopub.execute_input":"2023-06-14T17:45:22.008496Z","iopub.status.idle":"2023-06-14T17:45:22.014268Z","shell.execute_reply.started":"2023-06-14T17:45:22.008461Z","shell.execute_reply":"2023-06-14T17:45:22.013312Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def tokenizer(sentence):\n    regex = re.compile(r'(\\W+)')\n    tokens = regex.split(sentence.lower())\n    tokens = [w.strip() for w in tokens if len(w.strip()) > 0]\n    return tokens[:-1]","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:24.846978Z","iopub.execute_input":"2023-06-14T17:45:24.847919Z","iopub.status.idle":"2023-06-14T17:45:24.853976Z","shell.execute_reply.started":"2023-06-14T17:45:24.847884Z","shell.execute_reply":"2023-06-14T17:45:24.852859Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def load_question(question):\n  qu_tokens = tokenizer(question)\n  qu2idx = np.full(max_qu_length, question_vocab.word2idx(''))\n  qu2idx[:len(qu_tokens)] = [question_vocab.word2idx(token) for token in qu_tokens]\n  return qu2idx","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:27.047000Z","iopub.execute_input":"2023-06-14T17:45:27.047464Z","iopub.status.idle":"2023-06-14T17:45:27.052784Z","shell.execute_reply.started":"2023-06-14T17:45:27.047429Z","shell.execute_reply":"2023-06-14T17:45:27.051899Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def load_answer(answer):\n  answer_idx = answer_vocab.word2idx(answer)\n  answer = np.zeros(answer_vocab.vocab_size)\n  answer[answer_idx] = 1\n  return answer","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:31.489075Z","iopub.execute_input":"2023-06-14T17:45:31.489471Z","iopub.status.idle":"2023-06-14T17:45:31.494677Z","shell.execute_reply.started":"2023-06-14T17:45:31.489442Z","shell.execute_reply":"2023-06-14T17:45:31.493696Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def preprocess(features_path, question, answer):\n  features_path = features_path.numpy().decode('utf-8')\n  question = question.numpy().decode('utf-8')\n  answer = answer.numpy().decode('utf-8')\n\n  features = load_features(features_path)\n  question_vector = load_question(question)\n  answer_vector = load_answer(answer)\n\n  return (features, question_vector, answer_vector)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:33.488203Z","iopub.execute_input":"2023-06-14T17:45:33.488587Z","iopub.status.idle":"2023-06-14T17:45:33.495863Z","shell.execute_reply.started":"2023-06-14T17:45:33.488558Z","shell.execute_reply":"2023-06-14T17:45:33.494912Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def get_tensors_ready(x,y,z):\n  x.set_shape((49,512))\n  y.set_shape((30,))\n  z.set_shape((1000,))\n  return ((x,y),z)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:35.616981Z","iopub.execute_input":"2023-06-14T17:45:35.617651Z","iopub.status.idle":"2023-06-14T17:45:35.622480Z","shell.execute_reply.started":"2023-06-14T17:45:35.617615Z","shell.execute_reply":"2023-06-14T17:45:35.621574Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR='/kaggle/input/vqa-inputs'","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:50:20.944977Z","iopub.execute_input":"2023-06-14T17:50:20.945407Z","iopub.status.idle":"2023-06-14T17:50:20.949927Z","shell.execute_reply.started":"2023-06-14T17:50:20.945374Z","shell.execute_reply":"2023-06-14T17:50:20.948609Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def build_dataset(file_name):\n  data_dir = os.path.join(INPUT_DIR, file_name)\n  data = np.load(data_dir, allow_pickle=True).tolist()\n  if 'train' in file_name:\n    features_path = '/kaggle/working/VQA_preprocessed/ImagesFeatures/train/'\n  elif 'val' in file_name:\n    features_path = '/kaggle/working/VQA_preprocessed/ImagesFeatures/test/'\n  features = []\n  questions = []\n  answers = []\n  for element in data:\n    features.append(os.path.join(features_path, element[0][:-3] + 'npy'))\n    questions.append(element[1])\n    answers.append(element[2])\n\n  dataset = tf.data.Dataset.from_tensor_slices((features, questions, answers))\n  BATCH_SIZE = 128\n  dataset = dataset.cache()\n  dataset = dataset.map(lambda x, y, z: tf.py_function(func=preprocess,inp=[x, y, z], Tout=(tf.float32,tf.int32,tf.int32)), num_parallel_calls=tf.data.AUTOTUNE)\n  dataset = dataset.map(get_tensors_ready)\n  dataset = dataset.batch(BATCH_SIZE)\n  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n  return dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:50:22.748389Z","iopub.execute_input":"2023-06-14T17:50:22.748742Z","iopub.status.idle":"2023-06-14T17:50:22.759004Z","shell.execute_reply.started":"2023-06-14T17:50:22.748713Z","shell.execute_reply":"2023-06-14T17:50:22.758008Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"embedding_dict = {}\naverage_vec = np.zeros(300, \"float32\")\nn = 0\nwith open(\"/kaggle/input/glove6b600d/glove.6B.300d.txt\", 'r') as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1: ], \"float32\")\n    embedding_dict[word] = vector\n    average_vec += vector\n    n += 1\n\nvocab_size = question_vocab.vocab_size\nembedding_matrix = np.zeros((vocab_size, 300))\n\nfor i, word in enumerate(question_vocab.vocab):\n  if i < vocab_size:\n    emb_vector = embedding_dict.get(word)\n    if emb_vector is not None:\n      embedding_matrix[i] = emb_vector\n\naverage_vec = average_vec / n\nembedding_matrix[1] = average_vec   #giving unkown words the average value of the words embeddings","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:45:43.311609Z","iopub.execute_input":"2023-06-14T17:45:43.311956Z","iopub.status.idle":"2023-06-14T17:46:24.629792Z","shell.execute_reply.started":"2023-06-14T17:45:43.311926Z","shell.execute_reply":"2023-06-14T17:46:24.628496Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#image model\nim_input = tf.keras.layers.Input(shape=(49, 512))\nx1 = tf.keras.layers.Flatten()(im_input)\nx1 = tf.keras.layers.Dense(1024, activation='tanh')(x1)\n\n#question model\nvocab_size = question_vocab.vocab_size\nq_input = tf.keras.layers.Input(shape=max_qu_length)\nx2 = tf.keras.layers.Embedding(input_dim=vocab_size,\n                               output_dim=300,\n                               input_length=max_qu_length,\n                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                               trainable=True)(q_input)\n#x2 = tf.keras.layers.SimpleRNN(2048, return_sequences=True)(x2)\n_, state_h, state_c = tf.keras.layers.LSTM(512, return_state=True)(x2)\nx2 = tf.keras.layers.concatenate([state_h, state_c])\n\n#combine features\nout = tf.keras.layers.Multiply()([x1, x2])\n\n#model output\nnum_answers = answer_vocab.vocab_size\nout = tf.keras.layers.Dense(num_answers, activation='tanh')(out)\nout = tf.keras.layers.Dropout(0.5)(out)\nout = tf.keras.layers.Dense(num_answers, activation='tanh')(out)\nout = tf.keras.layers.Dropout(0.5)(out)\nout = tf.keras.layers.Dense(num_answers, activation='softmax')(out)\n\n#model specs\nmodel = tf.keras.Model(inputs=[im_input, q_input], outputs=[out])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n               metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:50:00.252644Z","iopub.execute_input":"2023-06-14T17:50:00.253013Z","iopub.status.idle":"2023-06-14T17:50:00.788162Z","shell.execute_reply.started":"2023-06-14T17:50:00.252982Z","shell.execute_reply":"2023-06-14T17:50:00.787152Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"!mkdir '/kaggle/working/checkpoints_features'\nmodel_checkpoint_path = \"/kaggle/working/checkpoints_features/checkpoint.h5\"\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path)\n\ntraining_history_path = \"/kaggle/working/checkpoints_features/history.log\"\nhistory_callback = tf.keras.callbacks.CSVLogger(training_history_path, append=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T17:59:55.454922Z","iopub.execute_input":"2023-06-14T17:59:55.455616Z","iopub.status.idle":"2023-06-14T17:59:56.551691Z","shell.execute_reply.started":"2023-06-14T17:59:55.455577Z","shell.execute_reply":"2023-06-14T17:59:56.550509Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/kaggle/working/checkpoints_features’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = build_dataset('train.npy')\n#val_dataset = build_dataset('val.npy')\nhistory = model.fit(train_dataset,\n                    epochs=50,\n                    shuffle=True,\n                    #validation_data = train_dataset,\n                    \n                    callbacks = [model_checkpoint, history_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-14T18:00:02.403272Z","iopub.execute_input":"2023-06-14T18:00:02.403668Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n3467/3467 [==============================] - 864s 249ms/step - loss: 2.1173 - accuracy: 0.3844\nEpoch 2/50\n3467/3467 [==============================] - 870s 251ms/step - loss: 1.9408 - accuracy: 0.4053\nEpoch 3/50\n3467/3467 [==============================] - 877s 253ms/step - loss: 1.8561 - accuracy: 0.4200\nEpoch 4/50\n3467/3467 [==============================] - 870s 251ms/step - loss: 1.7973 - accuracy: 0.4302\nEpoch 5/50\n3467/3467 [==============================] - 871s 251ms/step - loss: 1.7504 - accuracy: 0.4379\nEpoch 6/50\n3467/3467 [==============================] - 878s 253ms/step - loss: 1.7050 - accuracy: 0.4483\nEpoch 7/50\n3467/3467 [==============================] - 880s 254ms/step - loss: 1.6598 - accuracy: 0.4598\nEpoch 8/50\n3467/3467 [==============================] - 873s 252ms/step - loss: 1.6169 - accuracy: 0.4710\nEpoch 9/50\n3467/3467 [==============================] - 877s 253ms/step - loss: 1.5765 - accuracy: 0.4833\nEpoch 10/50\n3467/3467 [==============================] - 881s 254ms/step - loss: 1.5306 - accuracy: 0.4980\nEpoch 11/50\n3467/3467 [==============================] - 882s 254ms/step - loss: 1.4914 - accuracy: 0.5118\nEpoch 12/50\n3467/3467 [==============================] - 877s 253ms/step - loss: 1.4554 - accuracy: 0.5245\nEpoch 13/50\n3467/3467 [==============================] - 875s 252ms/step - loss: 1.4188 - accuracy: 0.5374\nEpoch 14/50\n3467/3467 [==============================] - 895s 258ms/step - loss: 1.3867 - accuracy: 0.5476\nEpoch 15/50\n3467/3467 [==============================] - 887s 256ms/step - loss: 1.3601 - accuracy: 0.5575\nEpoch 16/50\n3467/3467 [==============================] - 891s 257ms/step - loss: 1.3324 - accuracy: 0.5674\nEpoch 17/50\n3467/3467 [==============================] - 893s 258ms/step - loss: 1.3063 - accuracy: 0.5757\nEpoch 18/50\n3467/3467 [==============================] - 873s 252ms/step - loss: 1.2849 - accuracy: 0.5841\nEpoch 19/50\n3467/3467 [==============================] - 880s 254ms/step - loss: 1.2661 - accuracy: 0.5901\nEpoch 20/50\n3467/3467 [==============================] - 879s 254ms/step - loss: 1.2484 - accuracy: 0.5963\nEpoch 21/50\n3467/3467 [==============================] - 880s 254ms/step - loss: 1.2346 - accuracy: 0.6019\nEpoch 22/50\n3467/3467 [==============================] - 889s 257ms/step - loss: 1.2179 - accuracy: 0.6068\nEpoch 23/50\n3467/3467 [==============================] - 887s 256ms/step - loss: 1.2031 - accuracy: 0.6122\nEpoch 24/50\n3467/3467 [==============================] - 876s 253ms/step - loss: 1.1862 - accuracy: 0.6181\nEpoch 25/50\n3467/3467 [==============================] - 882s 254ms/step - loss: 1.1771 - accuracy: 0.6208\nEpoch 26/50\n3467/3467 [==============================] - 884s 255ms/step - loss: 1.1663 - accuracy: 0.6248\nEpoch 27/50\n3467/3467 [==============================] - 891s 257ms/step - loss: 1.1527 - accuracy: 0.6290\nEpoch 28/50\n3467/3467 [==============================] - 885s 255ms/step - loss: 1.1438 - accuracy: 0.6323\nEpoch 29/50\n3467/3467 [==============================] - 883s 255ms/step - loss: 1.1368 - accuracy: 0.6343\nEpoch 30/50\n3467/3467 [==============================] - 902s 260ms/step - loss: 1.1272 - accuracy: 0.6377\nEpoch 31/50\n3467/3467 [==============================] - 887s 256ms/step - loss: 1.1222 - accuracy: 0.6398\nEpoch 32/50\n3467/3467 [==============================] - 888s 256ms/step - loss: 1.1189 - accuracy: 0.6403\nEpoch 33/50\n3467/3467 [==============================] - 893s 258ms/step - loss: 1.1073 - accuracy: 0.6451\nEpoch 34/50\n3467/3467 [==============================] - 888s 256ms/step - loss: 1.1037 - accuracy: 0.6457\nEpoch 35/50\n3467/3467 [==============================] - 897s 259ms/step - loss: 1.1026 - accuracy: 0.6469\nEpoch 36/50\n3467/3467 [==============================] - 892s 257ms/step - loss: 1.0996 - accuracy: 0.6476\nEpoch 37/50\n3467/3467 [==============================] - 885s 255ms/step - loss: 1.0968 - accuracy: 0.6486\nEpoch 38/50\n3467/3467 [==============================] - 898s 259ms/step - loss: 1.0934 - accuracy: 0.6497\nEpoch 39/50\n3467/3467 [==============================] - 897s 259ms/step - loss: 1.0945 - accuracy: 0.6502\nEpoch 40/50\n3467/3467 [==============================] - 881s 254ms/step - loss: 1.0953 - accuracy: 0.6496\nEpoch 41/50\n3467/3467 [==============================] - 897s 259ms/step - loss: 1.0928 - accuracy: 0.6505\nEpoch 42/50\n3467/3467 [==============================] - 897s 259ms/step - loss: 1.0915 - accuracy: 0.6509\nEpoch 43/50\n3467/3467 [==============================] - 896s 258ms/step - loss: 1.0868 - accuracy: 0.6522\nEpoch 44/50\n3467/3467 [==============================] - 890s 257ms/step - loss: 1.0800 - accuracy: 0.6539\nEpoch 45/50\n3343/3467 [===========================>..] - ETA: 31s - loss: 1.0715 - accuracy: 0.6565","output_type":"stream"}]},{"cell_type":"code","source":"data = np.load('/content/drive/MyDrive/VQA_preprocessed/train.npy', allow_pickle=True).tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimg_name = data[10][0]\nquestion = data[10][1]\nans = data[10][2]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(question, ans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = load_question(question)\nans = load_answer(ans)\nimg = np.load(os.path.join('/content/Features/train', img_name[:-3] + 'npy'), allow_pickle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img =  tf.expand_dims(img, axis=0)\nquestion = tf.expand_dims(question, axis=0)\nans = tf.argmax(model.predict((img , question)), axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(answer_vocab.idx2word(ans[0]))","metadata":{},"execution_count":null,"outputs":[]}]}